{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fast_style_transfer_final_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gNzdSkaNFxq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "from tensorflow.keras.applications import vgg19\n",
        "\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Conv2DTranspose,BatchNormalization,Input,ReLU,Add,Lambda,LeakyReLU,Flatten\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import Tensor\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import keras\n",
        "\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trtG5qUsNdb4"
      },
      "source": [
        "autotune = tf.data.experimental.AUTOTUNE\n",
        "IMAGE_SIZE = [256, 256]\n",
        "BATCH_SIZE=16\n",
        "\n",
        "orig_img_size = (286, 286)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (256, 256, 3)\n",
        "# Weights initializer for the layers.\n",
        "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "# Gamma initializer for instance normalization.\n",
        "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "buffer_size = 256\n",
        "batch_size = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-diKhk1NvZb"
      },
      "source": [
        "\n",
        "dataset, _ = tfds.load(\"cycle_gan/cezanne2photo\", with_info=True, as_supervised=True)\n",
        "train_cezanne, train_photo = dataset[\"trainA\"], dataset[\"trainB\"]\n",
        "test_cezanne, test_photo = dataset[\"testA\"], dataset[\"testB\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "\n",
        "def preprocess_train_image(img, label):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.resize(img, [*orig_img_size])\n",
        "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "def preprocess_test_image(img, label):\n",
        "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
        "    img = normalize_img(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc5as9Y1Nz4k"
      },
      "source": [
        "\n",
        "# Apply the preprocessing operations to the training data\n",
        "train_photo = (\n",
        "    train_photo.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "train_cezanne = (\n",
        "    train_cezanne.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing operations to the test data\n",
        "test_photo = (\n",
        "    test_photo.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "test_cezanne = (\n",
        "    test_cezanne.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUWUShV-N5t-"
      },
      "source": [
        "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
        "for i, samples in enumerate(zip(train_photo.take(4), train_photo.take(4))):\n",
        "    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
        "    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
        "    ax[i, 0].imshow(horse)\n",
        "    ax[i, 1].imshow(horse)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXZ7xH_GR_Rk"
      },
      "source": [
        "# LOAD STYLE IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUt5QIySBDL"
      },
      "source": [
        "style_img = plt.imread('/content/image_style.jpg')\n",
        "\n",
        "style_img=cv2.resize(style_img,(256,256))\n",
        "\n",
        "\n",
        "style_img= normalize_img(style_img)\n",
        "plt.figure()\n",
        "plt.imshow(style_img)\n",
        "style_img=np.array(style_img)\n",
        "\n",
        "style_image=style_img.reshape(1,256,256,3)\n",
        "print(style_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ry98fxhNlEK"
      },
      "source": [
        "vgg=vgg19.VGG19(weights='imagenet',include_top=False)\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmWEtVrzNxwi"
      },
      "source": [
        "content_layers=['block4_conv2']\n",
        "style_layers=['block1_conv1',\n",
        "            'block2_conv1',\n",
        "            'block3_conv1',\n",
        "            'block4_conv1',\n",
        "            'block5_conv1']\n",
        "content_layers_weights=[1]\n",
        "style_layers_weights=[1]*5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUiiaUE-YAi0"
      },
      "source": [
        "LOSS MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su5wsZXQTKos"
      },
      "source": [
        "class LossNetwork:\n",
        "  def __init__(self,vgg,output_layer):\n",
        "    self.initial_model = vgg\n",
        "    self.output_layer = output_layer\n",
        "    self.model = self.construct_model()\n",
        "  \n",
        "  def construct_model(self):\n",
        "    output= self.initial_model.get_layer(self.output_layer).output\n",
        "    input = self.initial_model.input\n",
        "    model=Model(inputs=input,outputs=output)\n",
        "    return model\n",
        "  \n",
        "  def get_model(self):\n",
        "    return(self.model)\n",
        "\n",
        "  def get_output(self,input):\n",
        "    ## output to 0-255\n",
        "    input=(input+1)*127.5\n",
        "    outputs=self.model(vgg19.preprocess_input(input))\n",
        "    return(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nW5hY3tT3hk"
      },
      "source": [
        "model_style_1 = LossNetwork(vgg,'block1_conv1')\n",
        "model_style_2 = LossNetwork(vgg,'block2_conv1')\n",
        "model_style_3 = LossNetwork(vgg,'block3_conv1')\n",
        "model_style_4 = LossNetwork(vgg,'block4_conv1')\n",
        "model_style_5 = LossNetwork(vgg,'block5_conv1')\n",
        "\n",
        "model_feature = LossNetwork(vgg,'block4_conv2')\n",
        "\n",
        "#model=model_style_5.get_model()\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndcqVm3mIn-c"
      },
      "source": [
        "output_style_1 = model_style_1.get_output(style_image)\n",
        "output_style_2 = model_style_2.get_output(style_image)\n",
        "output_style_3 = model_style_3.get_output(style_image)\n",
        "output_style_4 = model_style_4.get_output(style_image)\n",
        "output_style_5 = model_style_5.get_output(style_image)\n",
        "\n",
        "output_feature = model_feature.get_output(style_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEyb58dsYEdZ"
      },
      "source": [
        "LOSS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssW7GDERYVvc"
      },
      "source": [
        "Content loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGk2Y0o0X75a"
      },
      "source": [
        "def content_loss(x,y):\n",
        "    return tf.reduce_mean(tf.square(x-y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vrpStGsYXpC"
      },
      "source": [
        "Style loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kIp_iOgYU79"
      },
      "source": [
        "def gram_matrix(x):\n",
        "    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
        "    return gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)\n",
        "\n",
        "def style_loss(x,y):\n",
        "    s=gram_matrix(x)\n",
        "    p=gram_matrix(y)\n",
        "    return tf.reduce_mean(tf.square(s-p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP04x30_LRkC"
      },
      "source": [
        "def preceptual_loss(real_image,style_image,predicted_image):\n",
        "    \n",
        "    ## Calculate output for real image \n",
        "    output_feature_real_image = model_feature.get_output(real_image)\n",
        "\n",
        "    ## Calculate output for style image \n",
        "    output_style_style_image_1 = model_style_1.get_output(style_image)\n",
        "    output_style_style_image_2 = model_style_2.get_output(style_image)\n",
        "    output_style_style_image_3 = model_style_3.get_output(style_image)\n",
        "    output_style_style_image_4 = model_style_4.get_output(style_image)\n",
        "    output_style_style_image_5 = model_style_5.get_output(style_image)\n",
        "\n",
        "    ## Calculate output for predicted image \n",
        "    output_style_predicted_image_1 = model_style_1.get_output(predicted_image)\n",
        "    output_style_predicted_image_2 = model_style_2.get_output(predicted_image)\n",
        "    output_style_predicted_image_3 = model_style_3.get_output(predicted_image)\n",
        "    output_style_predicted_image_4 = model_style_4.get_output(predicted_image)\n",
        "    output_style_predicted_image_5 = model_style_5.get_output(predicted_image)\n",
        "\n",
        "    output_feature_predicted_image = model_feature.get_output(predicted_image)\n",
        "\n",
        "    ## calculate content loss\n",
        "    content_loss_1 = content_loss(output_feature_real_image,output_feature_predicted_image)\n",
        "    \n",
        "    ## calculate style losse\n",
        "    \n",
        "    style_loss_1 = style_loss(output_style_style_image_1,output_style_predicted_image_1)\n",
        "    style_loss_2 = style_loss(output_style_style_image_2,output_style_predicted_image_2)\n",
        "    style_loss_3 = style_loss(output_style_style_image_3,output_style_predicted_image_3)\n",
        "    style_loss_4 = style_loss(output_style_style_image_4,output_style_predicted_image_4)\n",
        "    style_loss_5 = style_loss(output_style_style_image_5,output_style_predicted_image_5)\n",
        "    ## total loss\n",
        "    \n",
        "    c_loss=content_loss_1\n",
        "\n",
        "    s_loss=style_loss_1+style_loss_2+style_loss_3+style_loss_4+style_loss_5\n",
        "    #s_loss=s_loss*style_weight\n",
        "    return c_loss+s_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffJ-deQJX988"
      },
      "source": [
        "## MODEL gen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecr5NWPeO3wR"
      },
      "source": [
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "def residual_block(x: Tensor) -> Tensor:\n",
        "  initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
        "  x_0=x\n",
        "  x = tf.pad(x_0, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "  y = Conv2D(kernel_size=(3,3),strides= (1,1),filters=256,padding=\"valid\", use_bias=False,kernel_initializer=initializer)(x)\n",
        "  y = BatchNormalization(gamma_initializer=gamma_initializer)(y)\n",
        "  y = ReLU()(y)\n",
        "\n",
        "  y = tf.pad(y, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "  y = Conv2D(kernel_size=(3,3),strides=(1,1),filters=256,padding=\"valid\", use_bias=False,kernel_initializer=initializer)(y)\n",
        "  y = BatchNormalization(gamma_initializer=gamma_initializer)(y)\n",
        "\n",
        "  out = Add()([x_0,y])\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CulrgWYPTj6"
      },
      "source": [
        "def create_generator():\n",
        "  inputs = Input(shape=(256, 256, 3))\n",
        "  initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
        "  ## padding \n",
        "  pad= tf.pad(inputs, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "  ## C7s1-64\n",
        "  layer_1 = Conv2D(filters=64,kernel_size=(7,7),strides=(1,1),padding='valid',kernel_initializer=initializer,use_bias=False)(pad)\n",
        "  layer_1 = BatchNormalization(gamma_initializer=gamma_initializer)(layer_1)\n",
        "  layer_1 = ReLU()(layer_1)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  ## d128\n",
        "  layer_2 = Conv2D(filters=128,use_bias=False,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=initializer)(layer_1)\n",
        "  layer_2 = BatchNormalization(gamma_initializer=gamma_initializer)(layer_2)\n",
        "  layer_2 = ReLU()(layer_2)\n",
        "  \n",
        "\n",
        "  ## d256\n",
        "  layer_3 = Conv2D(filters=256,use_bias=False,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=initializer)(layer_2)\n",
        "  layer_3 = BatchNormalization(gamma_initializer=gamma_initializer)(layer_3)\n",
        "  layer_3 = ReLU()(layer_3)\n",
        "  \n",
        "  \n",
        "  \n",
        "  ## R256 - 1\n",
        "  layer_4 = residual_block(layer_3)\n",
        "  ## R256 - 2\n",
        "  layer_5 = residual_block(layer_4)\n",
        "  ## R256 - 3\n",
        "  layer_6 = residual_block(layer_5)\n",
        "  ## R256 - 4\n",
        "  layer_7 = residual_block(layer_6)\n",
        "   ## R256 - 5\n",
        "  layer_8 = residual_block(layer_7)\n",
        "  ## R256 - 6\n",
        "  layer_9 = residual_block(layer_8)\n",
        "  ## R256 - 7 \n",
        "  layer_10 = residual_block(layer_9)\n",
        "  ## R256 - 8\n",
        "  layer_11 = residual_block(layer_10)\n",
        "  ## R256 - 79\n",
        "  layer_12 = residual_block(layer_11)\n",
        "\n",
        "  ## u128\n",
        "  layer_13 = Conv2DTranspose(filters=128,use_bias=False,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=initializer)(layer_12)\n",
        "  layer_13 = BatchNormalization()(layer_13)\n",
        "  layer_13 = ReLU()(layer_13)\n",
        "  \n",
        "\n",
        "  ## u64\n",
        "  layer_14 = Conv2DTranspose(filters=64,use_bias=False,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=initializer)(layer_13)\n",
        "  layer_14 = BatchNormalization()(layer_14)\n",
        "  layer_14 = ReLU()(layer_14)\n",
        "  \n",
        "\n",
        "  ##c7s1-3\n",
        "  layer_15= tf.pad(layer_14, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "  layer_15 = Conv2D(padding='same',filters=3,kernel_size=(7,7),strides=(1,1),activation='tanh',kernel_initializer=initializer)(layer_15)\n",
        "\n",
        "\n",
        "  model = Model(inputs, layer_15)\n",
        "  return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3lha8MFVbAp"
      },
      "source": [
        "class FastStyleTransferModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator,\n",
        "        image_style\n",
        "  \n",
        "    ):\n",
        "        super(FastStyleTransferModel, self).__init__()\n",
        "        self.generator=generator\n",
        "        self.image_style=image_style\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        generator_optimizer,\n",
        "        generator_loss_fn,\n",
        " \n",
        "    ):\n",
        "        super(FastStyleTransferModel, self).compile()\n",
        "        self.generator_optimizer = generator_optimizer\n",
        "        self.generator_loss_fn=generator_loss_fn\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "      \n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted_image=self.generator(batch_data, training=True)\n",
        "            loss=self.generator_loss_fn(batch_data,self.image_style,predicted_image)\n",
        "\n",
        "           \n",
        "\n",
        "        grads_generator = tape.gradient(loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generators\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(\n",
        "            zip(grads_generator, self.generator.trainable_variables)\n",
        "        )\n",
        "        \n",
        "\n",
        "        return {\n",
        "            \"Perceptual loss\": loss,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQWIIajYXEP6"
      },
      "source": [
        "my_gen = create_generator()\n",
        "my_optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "# Create cycle gan model\n",
        "model = FastStyleTransferModel(\n",
        "    generator= my_gen,\n",
        "    image_style =style_image\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    generator_optimizer=my_optimizer,\n",
        "    generator_loss_fn=preceptual_loss\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyYw3gngXisn",
        "outputId": "d7a69636-a544-459d-9819-08b2b1a417de"
      },
      "source": [
        "model.fit(\n",
        "    x=train_photo,\n",
        "    epochs=4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1572/1572 [==============================] - 989s 629ms/step - Perceptual loss: 373417.7753\n",
            "Epoch 2/4\n",
            "1572/1572 [==============================] - 991s 630ms/step - Perceptual loss: 368007.5075\n",
            "Epoch 3/4\n",
            "1572/1572 [==============================] - 987s 628ms/step - Perceptual loss: 377694.9905\n",
            "Epoch 4/4\n",
            "1572/1572 [==============================] - 986s 627ms/step - Perceptual loss: 359116.6416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa832eba58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsXzvoLSv1lf"
      },
      "source": [
        "model.save_weights(\"fast_style_transfer_weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-9lJ_tjMvyM"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8KEOWqPmMveh"
      },
      "source": [
        "\n",
        "for img in test_photo:\n",
        "\n",
        "  \n",
        "  fake_painting=my_gen(img)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title('Original photo')\n",
        "  plt.imshow((img[0]+1)/2)\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title('Style applied')\n",
        "  plt.imshow((fake_painting[0]+1)/2)\n",
        "  \n",
        "  break\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}